"""
Ramadan 2026 â€“ Ù…Ø³Ù„Ø³Ù„Ø§Øª Ø¹Ø±Ø¨ÙŠØ© ÙÙ‚Ø·
Fetches images from series pages. Run with --serve to auto-update.
"""
import json, time, sys, os, re, random, threading
import http.server, socketserver, requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from datetime import datetime

BASE = "https://bx.alooytv6.xyz"
DATA_FILE = os.path.join(os.path.dirname(os.path.abspath(__file__)), "data.json")

# Ø¹Ø±Ø¨ÙŠ ÙÙ‚Ø·
KNOWN_SERIES = [
    ("Ø¨Ù†Øª Ø§Ù„Ù†Ø¹Ù…Ø§Ù†",                      "bint-al-noaman"),
    ("Ø§Ù„Ø®Ø±ÙˆØ¬ Ø¥Ù„Ù‰ Ø§Ù„Ø¨Ø¦Ø±",                 "al-khuroog-ila-al-ber"),
    ("Ø«Ø¹Ø§Ù„Ø¨ Ø§Ù„ØµØ­Ø±Ø§Ø¡",                    "thaealib-al-sahara"),
    ("Ø³Ø¬ÙˆÙ† Ø§Ù„Ø´ÙŠØ·Ø§Ù†",                     "sojun-alshaytan"),
    ("Ø¹Ù…Ø§Ø±Ø© Ø§Ù„Ø³Ø¹Ø§Ø¯Ø©",                    "omaret-el-saada"),
    ("Ø¨Ø¯Ù„ ØªØ§Ù„Ù",                         "badal-talef"),
    ("Ø£Ù†Ø§ ÙˆÙ‡ÙŠ ÙˆÙ‡ÙŠØ§",                     "ana-wa-heya-wa-haya"),
    ("Ø±Ø§Ù…Ø² Ù„ÙŠÙÙ„ Ø§Ù„ÙˆØ­Ø´",                  "ramez-level-el-wahsh"),
    ("Ø±ÙˆØ¬ Ø£Ø³ÙˆØ¯",                         "rouge-eswed"),
    ("Ø§Ù„Ø³Ø±Ø§ÙŠØ§ Ø§Ù„ØµÙØ±Ø§",                   "el-saraya-el-safra"),
    ("Ø´Ù…Ø³ Ø§Ù„Ø£ØµÙŠÙ„",                      "shams-el-aseel"),
    ("ÙŠØ§ Ø£Ù†Ø§ ÙŠØ§ Ù‡ÙŠ Ø¬2",                  "ya-ana-ya-heya-2"),
    ("Ø§Ù„ÙŠØªÙŠÙ…",                          "al-yateem"),
    ("Ø§Ù„Ø³ÙˆÙ‚ Ø§Ù„Ø­Ø±Ø©",                     "al-souq-al-hurra"),
    ("Ù…Ù†Ø§Ø¹Ø©",                           "mannaa"),
    ("Ù„ÙˆØ¨ÙŠ Ø§Ù„ØºØ±Ø§Ù…",                      "lubby-al-gharam"),
    ("Ø¹ÙŠÙ„Ø© Ø§Ù„Ù…Ù„Ùƒ",                      "elet-al-malek"),
    ("Ø§Ù„Ù†ÙˆÙŠÙ„Ø§ØªÙŠ",                        "al-noelati"),
    ("Ø§Ø³Ø£Ù„ Ø±ÙˆØ­Ùƒ",                       "esaal-rouhak"),
    ("Ø¨Ù†Ø§Øª Ø§Ù„Ø¹Ù… Ø¬2 : Ø§Ù†ØªÙ‚Ø§Ù… Ø§Ù„Ù…ÙˆØªÙ‰",     "banat-al-am-2"),
    ("Ø¹Ø±Ø´ Ø§Ù„Ø´ÙŠØ·Ø§Ù†",                     "arsh-al-shaytan"),
    ("Ø§Ù„Ù…ØµÙŠØ¯Ø©",                         "el-masyada"),
    ("Ø§Ù„Ù…Ø¯Ø§Ø­ Ø¬6: Ø£Ø³Ø·ÙˆØ±Ø© Ø§Ù„Ù†Ù‡Ø§ÙŠØ©",        "al-maddah-6-ostorat-al-nehaya"),
    ("Ù‚Ø·Ø± ØµØºÙ†Ø·ÙˆØ·",                      "atr-soghantoot"),
    ("Ø¹ÙŠÙ† Ø³Ø­Ø±ÙŠØ©",                       "ein-sehreya"),
    ("ÙƒØ§Ù† ÙŠØ§ Ù…ÙƒØ§Ù†",                     "kan-ya-makan"),
    ("Ø­ÙƒØ§ÙŠØ© Ù†Ø±Ø¬Ø³",                      "hekayet-narges"),
    ("Ø£ÙˆÙ„Ø§Ø¯ Ø§Ù„Ø±Ø§Ø¹ÙŠ",                     "awlad-el-raaey"),
    ("Ø­Ø¯ Ø£Ù‚ØµÙ‰",                         "had-aqsa"),
    ("Ø¨ÙŠØ¨Ùˆ",                            "bibo"),
    ("ØªÙˆØ§Ø¨Ø¹",                           "tawabea"),
    ("Ø±Ø£Ø³ Ø§Ù„Ø£ÙØ¹Ù‰",                      "ras-al-afaa"),
    ("Ø¯Ø±Ø´",                             "darsh"),
    ("Ø¨Ø§Ø¨Ø§ ÙˆÙ…Ø§Ù…Ø§ Ø¬ÙŠØ±Ø§Ù†",                 "baba-w-mama-giran"),
    ("Ø§Ù„Ù„ÙˆÙ† Ø§Ù„Ø£Ø²Ø±Ù‚",                    "al-lawn-al-azraq"),
    ("Ø³Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø¬Ù†ÙˆÙ†",                   "saadaet-al-magnoun"),
    ("ØµØ­Ø§Ø¨ Ø§Ù„Ø£Ø±Ø¶",                     "sohab-al-ard"),
    ("Ø³ÙˆØ§ Ø³ÙˆØ§",                         "sawa-sawa"),
    ("Ø¹Ø±Ø¶ ÙˆØ·Ù„Ø¨",                        "aard-w-talab"),
    ("Ø¹Ù„Ù‰ Ù‚Ø¯ Ø§Ù„Ø­Ø¨",                     "ala-add-el-hob"),
    ("ÙƒÙ„Ù‡Ù… Ø¨ÙŠØ­Ø¨ÙˆØ§ Ù…ÙˆØ¯ÙŠ",                 "kollohom-beehebbo-moody"),
    ("Ø¹Ù„ÙŠ ÙƒÙ„Ø§ÙŠ",                        "ali-clay"),
    ("ÙØ®Ø± Ø§Ù„Ø¯Ù„ØªØ§",                      "fakhr-el-delta"),
    ("ÙØ±ØµØ© Ø£Ø®ÙŠØ±Ø©",                      "forsa-akhira"),
    ("ÙÙ† Ø§Ù„Ø­Ø±Ø¨",                        "fan-al-harb"),
    ("Ø§ØªÙ†ÙŠÙ† ØºÙŠØ±Ù†Ø§",                     "etnen-gherna"),
    ("Ø£Ø¨ ÙˆÙ„ÙƒÙ†",                         "ab-wa-laken"),
    ("Ù…Ø·Ø¨Ø® Ø§Ù„Ù…Ø¯ÙŠÙ†Ø©",                    "matbakh-al-madinah"),
    ("Ù…ÙˆÙ„Ø§Ù†Ø§",                          "mawlana"),
    ("Ù† Ø§Ù„Ù†Ø³ÙˆØ©",                        "noon-el-neswa"),
]

USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:124.0) Gecko/20100101 Firefox/124.0",
]


def make_session():
    s = requests.Session()
    s.headers.update({
        "User-Agent": random.choice(USER_AGENTS),
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "ar,en-US;q=0.9,en;q=0.8",
        "Accept-Encoding": "identity",
        "Connection": "keep-alive",
        "Cache-Control": "max-age=0",
    })
    return s


session = make_session()


def fetch(url, retries=4):
    global session
    for attempt in range(retries):
        try:
            r = session.get(url, timeout=25)
            if r.status_code in (521, 522, 523, 524):
                wait = 6 + attempt * 4 + random.uniform(1, 3)
                print(f"  [CF-{r.status_code}] retrying in {wait:.1f}s...")
                session = make_session()
                time.sleep(wait)
                continue
            r.raise_for_status()
            return r.text
        except requests.exceptions.HTTPError as e:
            code = e.response.status_code if e.response else 0
            if attempt < retries - 1:
                time.sleep(5 + attempt * 5)
                session = make_session()
            else:
                raise
        except requests.exceptions.RequestException:
            if attempt < retries - 1:
                time.sleep(5 + attempt * 4)
            else:
                raise
    raise RuntimeError(f"Failed: {url}")


def get_image(html):
    soup = BeautifulSoup(html, "html.parser")
    og = soup.find("meta", property="og:image")
    if og:
        src = og.get("content", "")
        if src and "blank" not in src:
            return src if src.startswith("http") else urljoin(BASE, src)
    for img in soup.find_all("img"):
        src = img.get("data-src") or img.get("src") or ""
        if "video_thumb" in src:
            return src if src.startswith("http") else urljoin(BASE, src)
    return ""


def get_description(html):
    soup = BeautifulSoup(html, "html.parser")
    for sel in [".description", ".film-description", ".dp-i-c-des", "[itemprop='description']"]:
        d = soup.select_one(sel)
        if d:
            txt = d.get_text(strip=True)
            if txt and len(txt) > 10:
                return txt[:300]
    og = soup.find("meta", property="og:description")
    if og:
        return og.get("content", "")[:300]
    return ""


def scrape_all():
    existing = {}
    if os.path.exists(DATA_FILE):
        with open(DATA_FILE, encoding="utf-8") as f:
            try:
                old = json.load(f)
                for s in old.get("series", []):
                    existing[s["link"]] = s
            except Exception:
                pass

    all_series = []

    for title, slug in KNOWN_SERIES:
        url = f"{BASE}/watch/{slug}.html"
        cached = existing.get(url, {})

        print(f"  â–¶ {title}")

        image = cached.get("image", "")
        description = cached.get("description", "")

        if not image:
            try:
                html = fetch(url)
                image = get_image(html)
                if not description:
                    description = get_description(html)
                time.sleep(random.uniform(0.25, 0.6))
            except Exception as e:
                print(f"    [WARN] {e}")

        all_series.append({
            "title": title,
            "link": url,
            "image": image,
            "description": description,
            "quality": cached.get("quality", "HD"),
        })

    return {
        "last_updated": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "total": len(all_series),
        "series": all_series,
    }


def save_data(data):
    with open(DATA_FILE, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    print(f"\nâœ… Saved {data['total']} series to data.json")
    print(f"   Last updated: {data['last_updated']}")


def run_scraper():
    print("\nğŸŒ™ Ø±Ù…Ø¶Ø§Ù† 2026 â€” Scraper")
    print("=" * 50)
    data = scrape_all()
    save_data(data)
    return data


def auto_update_loop(interval_minutes=30):
    while True:
        try:
            run_scraper()
        except Exception as e:
            print(f"\n[ERROR] {e}")
        print(f"\nâ° Next update in {interval_minutes} minutes...")
        time.sleep(interval_minutes * 60)


def serve(port=8000):
    directory = os.path.dirname(os.path.abspath(__file__))
    run_scraper()
    updater = threading.Thread(target=auto_update_loop, args=(30,), daemon=True)
    updater.start()
    os.chdir(directory)
    handler = http.server.SimpleHTTPRequestHandler

    class QuietHandler(handler):
        def log_message(self, format, *args):
            pass  # Silence request logs

    with socketserver.TCPServer(("", port), QuietHandler) as httpd:
        print(f"\nğŸš€ http://localhost:{port}")
        print(f"   Auto-updates every 30 min\n")
        try:
            httpd.serve_forever()
        except KeyboardInterrupt:
            print("\nğŸ‘‹ Stopped.")


if __name__ == "__main__":
    if "--serve" in sys.argv:
        port = 8000
        for i, arg in enumerate(sys.argv):
            if arg == "--port" and i + 1 < len(sys.argv):
                port = int(sys.argv[i + 1])
        serve(port)
    else:
        run_scraper()
